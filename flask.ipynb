{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, jsonify, render_template\n",
    "import random\n",
    "import requests\n",
    "import tensorflow\n",
    "import csv\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "import numpy as np\n",
    "import librosa \n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "dependencies = {'f1_m': f1_m }\n",
    "# create a custom function to load model\n",
    "def load_all_models(n_models):\n",
    "    all_models = list()\n",
    "    for i in range(n_models):\n",
    "\t\t# Specify the filename \n",
    "        filename = 'Ensemble_Models\\model'+ str(i + 1) + '.h5'\n",
    "        # load the model \n",
    "        model = load_model(filename,custom_objects=dependencies)\n",
    "\t\t# Add a list of all the weaker learners\n",
    "        all_models.append(model)\n",
    "        print('>loaded %s' % filename)\n",
    "    return all_models\n",
    "\n",
    "# create stacked model input dataset as outputs from the ensemble\n",
    "def stacked_dataset(members, inputX):\n",
    "    stackX = None\n",
    "    for model in members:\n",
    "        # make prediction\n",
    "        yhat = model.predict(inputX, verbose=0)\n",
    "        # stack predictions into [rows, members, probabilities]\n",
    "        if stackX is None:\n",
    "            stackX = yhat #\n",
    "        else:\n",
    "            stackX = np.dstack((stackX, yhat))\n",
    "    # flatten predictions to [rows, members x probabilities]\n",
    "    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "    return stackX\n",
    "\n",
    "def fit_stacked_model(members, inputX, inputy):\n",
    "    # create dataset using ensemble\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "    # fit the meta learner\n",
    "    model = LogisticRegression() #meta learner\n",
    "    model.fit(stackedX, inputy)\n",
    "    return model\n",
    "\n",
    "def stacked_prediction(members, model, inputX):\n",
    "    # create dataset using ensemble\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "    # make a prediction\n",
    "    yhat = model.predict(stackedX)\n",
    "    return yhat\n",
    "\n",
    "def noise(data):\n",
    "    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n",
    "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
    "    return data\n",
    "\n",
    "def feat_ext(data):\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=22050).T, axis=0)\n",
    "    return mfcc\n",
    "\n",
    "def get_feat(path):\n",
    "    data, sample_rate = librosa.load(path, duration=5, offset=0.6)\n",
    "    # normal data\n",
    "    res1 = feat_ext(data)\n",
    "    result = np.array(res1)\n",
    "    #data with noise\n",
    "    noise_data = noise(data)\n",
    "    res2 = feat_ext(noise_data)\n",
    "    result = np.vstack((result, res2))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded Ensemble_Models\\model1.h5\n",
      ">loaded Ensemble_Models\\model2.h5\n",
      ">loaded Ensemble_Models\\model3.h5\n",
      ">loaded Ensemble_Models\\model4.h5\n",
      "Loaded 4 models\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__, template_folder='template', static_folder='static')\n",
    "\n",
    "n_members = 4\n",
    "members = load_all_models(n_members)\n",
    "print('Loaded %d models' % len(members))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/predict_',methods=['GET','POST'])\n",
    "def predict_():\n",
    "\n",
    "    audio_file= r\"C:\\Users\\hp\\Downloads\\output.wav\"\n",
    "\n",
    "    r = sr.Recognizer()\n",
    "    # open the file\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "    # listen for the data (load audio to memory)\n",
    "        r.adjust_for_ambient_noise(source)\n",
    "        audio_data = r.listen(source)\n",
    "    # recognize (convert from speech to text)\n",
    "        text = r.recognize_google(audio_data)\n",
    "        print(text.casefold())\n",
    "        speech_text= text.casefold()\n",
    "\n",
    "    if len(speech_text)>=2:\n",
    "\n",
    "        Features = pd.read_csv(r'D:\\SIH 22 STRATAGEM\\SIH\\features_Emergency.csv')\n",
    "        Y = Features['labels'].values\n",
    "        encoder = OneHotEncoder()\n",
    "        Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()\n",
    "        # splitting data\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, Y,test_size=0.3, random_state=0, shuffle=True)\n",
    "        \n",
    "        x_traincnn =np.expand_dims(x_train, axis=2)\n",
    "        x_testcnn= np.expand_dims(x_test, axis=2)\n",
    "        x_traincnn.shape, y_train.shape, x_testcnn.shape, y_test.shape\n",
    "\n",
    "       \n",
    "        y_test = encoder.inverse_transform(y_test)\n",
    "\n",
    "        model = fit_stacked_model(members,x_testcnn, y_test)\t\n",
    "        \n",
    "        feature = get_feat(audio_file)\n",
    "        test =np.expand_dims(feature, axis=2)\n",
    "        livepreds = stacked_prediction(members, model, test)\n",
    "        # livepredictions = (encoder.inverse_transform((livepreds)))\n",
    "\n",
    "        dataset = pd.read_csv(r\"D:\\SIH 22 STRATAGEM\\SIH\\Words_emotion.csv\")\n",
    "        dataset.astype({'Phrases':'string', 'Emotions':'string'}).dtypes\n",
    "        X = dataset['Phrases']\n",
    "        y = dataset['Emotions']\n",
    "\n",
    "        \n",
    "\n",
    "        emotion_list=[]\n",
    "        for i in range(len(X)):\n",
    "            if X[i] in speech_text:\n",
    "                emotion_list.append(y[i])  \n",
    "        \n",
    "        if emotion_list:\n",
    "            count_angry=emotion_list.count(\"abusive\")\n",
    "            count_notprank=emotion_list.count(\"Not_Prank\")\n",
    "            #count_painful=emotion_list.count(\"painful\")\n",
    "            #count_stressful=emotion_list.count(\"stressful\")\n",
    "            count_prank=emotion_list.count(\"prank\")\n",
    "            total = count_prank+count_angry+count_notprank\n",
    "            angry_percentage = count_angry/total*100\n",
    "            prank_percentage = count_prank/total*100\n",
    "            notprank_percentage = count_notprank/total*100\n",
    "            count_list = [angry_percentage,notprank_percentage ,prank_percentage]\n",
    "            for ind, i in enumerate(count_list):\n",
    "                count_list[ind] = \"{}%\".format(i)\n",
    "            list = ['Abusive','Not Prank','Prank']\n",
    "            df = pd.DataFrame(zip(list,count_list))\n",
    "        else:\n",
    "            count_list = [0,0,0]\n",
    "            for ind, i in enumerate(count_list):\n",
    "                [ind] = \"{}%\".format(i)\n",
    "            list = ['Abusive','Not Prank','Prank']\n",
    "            df = pd.DataFrame(zip(list,count_list))\n",
    "\n",
    "        final_df = df.sort_values(by=[0], ascending=False)\n",
    "\n",
    "        return render_template('result.html', prediction_text='{}'.format(livepreds[0]), text_emotion1='{}'.format(final_df.iloc[0,:].to_string(index=False,header=False)),text_emotion2='{}'.format(final_df.iloc[1,:].to_string(index=False,header=False)),text_emotion3='{}'.format(final_df.iloc[2,:].to_string(index=False,header=False)))\n",
    "    else:\n",
    "        return render_template('result.html', prediction_text='{}'.format(\"BLANK CALL\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @app.route('/results',methods=['POST'])\n",
    "# def results():\n",
    "#     data = request.get_json(force=True)\n",
    "#     prediction = model.predict(data.values())\n",
    "    \n",
    "#     return jsonify(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True,use_reloader=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "75d5fa5647ed718dff6db6e19cb5826cae172c65e0d9788c078b693ea2246e0d"
   }
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "fb4569285eef3a3450cb62085a5b1e0da4bce0af555edc33dcf29baf3acc1368"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
